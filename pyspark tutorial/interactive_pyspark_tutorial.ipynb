{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4fa3699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark version: 4.0.0\n",
      "python version: 3.12.1 (main, Jul 10 2025, 11:57:50) [GCC 13.3.0]\n",
      "os version: posix.uname_result(sysname='Linux', nodename='codespaces-780a6a', release='6.8.0-1030-azure', version='#35~22.04.1-Ubuntu SMP Mon May 26 18:08:30 UTC 2025', machine='x86_64')\n",
      "‚úÖ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# üöÄ Your Code Here - Exercise 1: Import Libraries\n",
    "# Import all the required libraries below:\n",
    "\n",
    "# Your imports here...\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Check versions here...\n",
    "print(\"pyspark version:\", pyspark.__version__)\n",
    "print(\"python version:\",sys.version)\n",
    "print(\"os version:\",os.uname())\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "210388ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.0\n",
      "Application name: My PySpark Learning Journey\n"
     ]
    }
   ],
   "source": [
    "# üöÄ Your Code Here - Exercise 2: Create SparkSession\n",
    "# Create your SparkSession with the specified configurations:\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"My PySpark Learning Journey\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"10MB\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"Spark version:\", spark.version)  \n",
    "print(\"Application name:\", spark.conf.get(\"spark.app.name\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a988b2",
   "metadata": {},
   "source": [
    "## üìù Exercise 3: Create Your First DataFrame\n",
    "\n",
    "**Your Mission:** Create a DataFrame with employee data using a proper schema.\n",
    "\n",
    "**The Data to Use:**\n",
    "```python\n",
    "employees_data = [\n",
    "    (1, \"Alice Johnson\", \"Data Engineering\", 85000, \"2021-01-15\"),\n",
    "    (2, \"Bob Smith\", \"Data Science\", 92000, \"2020-03-20\"),\n",
    "    (3, \"Carol Davis\", \"Analytics\", 78000, \"2021-06-10\"),\n",
    "    (4, \"David Wilson\", \"Engineering\", 95000, \"2019-11-05\"),\n",
    "    (5, \"Eva Brown\", \"Data Science\", 88000, \"2020-09-12\")\n",
    "]\n",
    "```\n",
    "\n",
    "**Your Tasks:**\n",
    "1. Define a schema with these fields:\n",
    "   - employee_id (IntegerType)\n",
    "   - name (StringType) \n",
    "   - department (StringType)\n",
    "   - salary (IntegerType)\n",
    "   - hire_date (StringType)\n",
    "\n",
    "2. Create the DataFrame using the schema\n",
    "3. Show the DataFrame\n",
    "4. Print the schema\n",
    "\n",
    "**Remember:** Use `StructType([StructField(...), ...])` for schema definition!\n",
    "\n",
    "**Start coding! üëá**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a45ee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Your Code Here - Exercise 3: Create DataFrame with Schema\n",
    "\n",
    "# Define your data\n",
    "employees_data = [\n",
    "    (1, \"Alice Johnson\", \"Data Engineering\", 85000, \"2021-01-15\"),\n",
    "    (2, \"Bob Smith\", \"Data Science\", 92000, \"2020-03-20\"),\n",
    "    (3, \"Carol Davis\", \"Analytics\", 78000, \"2021-06-10\"),\n",
    "    (4, \"David Wilson\", \"Engineering\", 95000, \"2019-11-05\"),\n",
    "    (5, \"Eva Brown\", \"Data Science\", 88000, \"2020-09-12\")\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# 1. Define your schema here:\n",
    "# employees_schema = StructType([\n",
    "#     StructField(\"employee_id\", IntegerType(), True),\n",
    "#     StructField(\"name\", StringType(), True),\n",
    "#     StructField(\"department\", StringType(), True),\n",
    "#     StructField(\"salary\", IntegerType(), True),\n",
    "#     StructField(\"hire_date\", StringType(), True)\n",
    "# ])\n",
    "\n",
    "# 2. Create DataFrame here:\n",
    "# employees_df = spark.createDataFrame(employees_data, employees_schema)\n",
    "\n",
    "# 3. Show the DataFrame and print schema:\n",
    "# employees_df.show()\n",
    "# employees_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d68c93",
   "metadata": {},
   "source": [
    "## üìù Exercise 4: Master All Join Types \n",
    "\n",
    "**The Big Challenge:** Now let's practice all join types! First, create a second DataFrame for performance data.\n",
    "\n",
    "**Create Performance DataFrame:**\n",
    "```python\n",
    "performance_data = [\n",
    "    (1, 95, \"Excellent\"),\n",
    "    (2, 87, \"Good\"),\n",
    "    (3, 92, \"Excellent\"), \n",
    "    (6, 88, \"Good\")  # Note: employee_id 6 doesn't exist in employees!\n",
    "]\n",
    "```\n",
    "\n",
    "**Your Mission:**\n",
    "1. Create a performance DataFrame with columns: employee_id, score, rating\n",
    "2. Then perform these joins with the employees DataFrame:\n",
    "   - **Inner Join** (only matching records)\n",
    "   - **Left Join** (all employees + matching performance)\n",
    "   - **Right Join** (all performance + matching employees) \n",
    "   - **Full Outer Join** (everything from both)\n",
    "   - **Semi Join** (employees who have performance records - no perf columns)\n",
    "   - **Anti Join** (employees who DON'T have performance records)\n",
    "\n",
    "**For each join:**\n",
    "- Print what the join does\n",
    "- Show the result\n",
    "- Print the count\n",
    "\n",
    "**Time to master joins! üîó**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0a5e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "üöÄ Your Code Here - Exercise 4: Practice All Join Types\n",
    "# 1. Create performance DataFrame first\n",
    "performance_data = [\n",
    "    (1, 95, \"Excellent\"),\n",
    "    (2, 87, \"Good\"),\n",
    "    (3, 92, \"Excellent\"), \n",
    "    (6, 88, \"Good\")  # Employee 6 doesn't exist in employees!\n",
    "]\n",
    "\n",
    "# Create performance DataFrame here:\n",
    "# performance_df = spark.createDataFrame(performance_data, [\"employee_id\", \"score\", \"rating\"])\n",
    "\n",
    "# 2. Now practice each join type:\n",
    "\n",
    "# INNER JOIN:\n",
    "print(\"üîó INNER JOIN - Only matching records:\")\n",
    "# inner_join = employees_df.join(performance_df, \"employee_id\", \"inner\")\n",
    "# inner_join.show()\n",
    "# print(f\"Result count: {inner_join.count()}\")\n",
    "\n",
    "# LEFT JOIN:\n",
    "print(\"\\nüîó LEFT JOIN - All employees + matching performance:\")\n",
    "# left_join = employees_df.join(performance_df, \"employee_id\", \"left\")\n",
    "# left_join.show()\n",
    "# print(f\"Result count: {left_join.count()}\")\n",
    "\n",
    "# RIGHT JOIN:\n",
    "print(\"\\nüîó RIGHT JOIN - All performance + matching employees:\")\n",
    "# right_join = employees_df.join(performance_df, \"employee_id\", \"right\")\n",
    "# right_join.show()\n",
    "# print(f\"Result count: {right_join.count()}\")\n",
    "\n",
    "# FULL OUTER JOIN:\n",
    "print(\"\\nüîó FULL OUTER JOIN - Everything from both tables:\")\n",
    "# full_join = employees_df.join(performance_df, \"employee_id\", \"outer\")\n",
    "# full_join.show()\n",
    "# print(f\"Result count: {full_join.count()}\")\n",
    "\n",
    "# SEMI JOIN:\n",
    "print(\"\\nüîó SEMI JOIN - Employees who have performance (no perf columns):\")\n",
    "# semi_join = employees_df.join(performance_df, \"employee_id\", \"semi\")\n",
    "# semi_join.show()\n",
    "# print(f\"Result count: {semi_join.count()}\")\n",
    "\n",
    "# ANTI JOIN:\n",
    "print(\"\\nüîó ANTI JOIN - Employees who DON'T have performance:\")\n",
    "# anti_join = employees_df.join(performance_df, \"employee_id\", \"anti\")\n",
    "# anti_join.show()\n",
    "# print(f\"Result count: {anti_join.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e81186",
   "metadata": {},
   "source": [
    "## üìù Exercise 5: Optimization Techniques Challenge\n",
    "\n",
    "**Advanced Challenge:** Let's optimize your joins and learn performance techniques!\n",
    "\n",
    "**Create a Department Lookup Table:**\n",
    "```python\n",
    "dept_data = [\n",
    "    (\"Data Engineering\", \"DE\", \"Technology\"),\n",
    "    (\"Data Science\", \"DS\", \"Technology\"),\n",
    "    (\"Analytics\", \"AN\", \"Business\"),\n",
    "    (\"Engineering\", \"ENG\", \"Technology\")\n",
    "]\n",
    "```\n",
    "\n",
    "**Your Optimization Tasks:**\n",
    "\n",
    "1. **Create Department DataFrame** with columns: dept_name, dept_code, category\n",
    "\n",
    "2. **Practice Broadcast Join:**\n",
    "   - Join employees with departments using broadcast hint\n",
    "   - Use `broadcast()` function for the small dept table\n",
    "   - Show the execution plan with `.explain()`\n",
    "\n",
    "3. **Sorting Challenge:**\n",
    "   - Sort employees by department, then salary (highest first)\n",
    "   - Sort with null handling (add a test record with null department)\n",
    "\n",
    "4. **Partitioning Practice:**\n",
    "   - Check current number of partitions\n",
    "   - Repartition by department\n",
    "   - Coalesce to fewer partitions\n",
    "\n",
    "5. **Configuration Check:**\n",
    "   - Print current values for these configs:\n",
    "     - `spark.sql.autoBroadcastJoinThreshold`\n",
    "     - `spark.sql.shuffle.partitions`\n",
    "     - `spark.sql.adaptive.enabled`\n",
    "\n",
    "**Ready for the optimization challenge? üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
