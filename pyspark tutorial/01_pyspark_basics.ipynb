{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "972e2e39",
   "metadata": {},
   "source": [
    "# PySpark Complete Tutorial - From Basics to Advanced\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "By the end of this notebook, you will master:\n",
    "- PySpark environment setup and configuration\n",
    "- All types of joins (Inner, Outer, Cross, Semi, Anti)\n",
    "- Advanced optimization techniques and performance tuning\n",
    "- Sorting strategies and partitioning methods\n",
    "- Memory management and Catalyst optimizer\n",
    "- Real-world data processing scenarios\n",
    "\n",
    "## ðŸ“š What We'll Cover Today\n",
    "1. **Environment Setup** - Installation and SparkSession configuration\n",
    "2. **DataFrame Fundamentals** - Creating, transforming, and manipulating data\n",
    "3. **Join Operations** - Complete coverage of all join types\n",
    "4. **Optimization Techniques** - Performance tuning and best practices\n",
    "5. **Advanced Topics** - UDFs, Window functions, and streaming\n",
    "\n",
    "Let's dive deep into PySpark! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3241fcb4",
   "metadata": {},
   "source": [
    "## 1. PySpark Environment Setup and SparkSession\n",
    "\n",
    "### Installation and Environment Configuration\n",
    "First, let's set up PySpark with all necessary configurations for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db29506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PySpark (run this if PySpark is not installed)\n",
    "# !pip install pyspark\n",
    "\n",
    "# Import essential libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check PySpark version\n",
    "print(f\"PySpark Version: {pyspark.__version__}\")\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "\n",
    "# Set environment variables for optimal performance\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d1bd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession with optimized configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Complete Tutorial\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"10MB\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce verbose output\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"SparkSession created successfully!\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Application Name: {spark.conf.get('spark.app.name')}\")\n",
    "print(f\"Master: {spark.conf.get('spark.master')}\")\n",
    "\n",
    "# Display key configurations\n",
    "print(\"\\nðŸ”§ Key Configurations:\")\n",
    "configs = [\n",
    "    \"spark.sql.adaptive.enabled\",\n",
    "    \"spark.sql.adaptive.coalescePartitions.enabled\", \n",
    "    \"spark.sql.autoBroadcastJoinThreshold\",\n",
    "    \"spark.sql.shuffle.partitions\"\n",
    "]\n",
    "\n",
    "for config in configs:\n",
    "    print(f\"{config}: {spark.conf.get(config)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada29db6",
   "metadata": {},
   "source": [
    "## 2. Creating DataFrames from Various Sources\n",
    "\n",
    "### Learn multiple ways to create DataFrames for different data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bcfc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Creating DataFrame from Python lists\n",
    "print(\"ðŸ“Š Creating DataFrames from various sources:\\n\")\n",
    "\n",
    "# Sample data for employees\n",
    "employees_data = [\n",
    "    (1, \"John Doe\", \"Engineering\", 75000, \"2020-01-15\"),\n",
    "    (2, \"Jane Smith\", \"Marketing\", 65000, \"2019-03-20\"),\n",
    "    (3, \"Mike Johnson\", \"Engineering\", 80000, \"2021-06-10\"),\n",
    "    (4, \"Sarah Wilson\", \"HR\", 55000, \"2018-11-05\"),\n",
    "    (5, \"David Brown\", \"Engineering\", 90000, \"2017-09-12\"),\n",
    "    (6, \"Lisa Davis\", \"Marketing\", 70000, \"2020-08-30\"),\n",
    "    (7, \"Tom Miller\", \"Finance\", 62000, \"2019-12-01\"),\n",
    "    (8, \"Amy Taylor\", \"Engineering\", 85000, \"2021-02-14\")\n",
    "]\n",
    "\n",
    "# Define schema for better performance and data quality\n",
    "employees_schema = StructType([\n",
    "    StructField(\"employee_id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"hire_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame with schema\n",
    "employees_df = spark.createDataFrame(employees_data, employees_schema)\n",
    "\n",
    "print(\"âœ… Employees DataFrame created:\")\n",
    "employees_df.show()\n",
    "employees_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77defb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Creating DataFrame from dictionary\n",
    "departments_dict = [\n",
    "    {\"dept_id\": 1, \"dept_name\": \"Engineering\", \"location\": \"San Francisco\", \"budget\": 1000000},\n",
    "    {\"dept_id\": 2, \"dept_name\": \"Marketing\", \"location\": \"New York\", \"budget\": 500000},\n",
    "    {\"dept_id\": 3, \"dept_name\": \"HR\", \"location\": \"Chicago\", \"budget\": 300000},\n",
    "    {\"dept_id\": 4, \"dept_name\": \"Finance\", \"location\": \"Boston\", \"budget\": 400000}\n",
    "]\n",
    "\n",
    "departments_df = spark.createDataFrame(departments_dict)\n",
    "print(\"âœ… Departments DataFrame created:\")\n",
    "departments_df.show()\n",
    "\n",
    "# 3. Creating DataFrame from CSV data (simulated)\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Create sample CSV data\n",
    "csv_data = \"\"\"employee_id,name,department,salary,hire_date\n",
    "1,John Doe,Engineering,75000,2020-01-15\n",
    "2,Jane Smith,Marketing,65000,2019-03-20\n",
    "3,Mike Johnson,Engineering,80000,2021-06-10\"\"\"\n",
    "\n",
    "# Write to temporary file\n",
    "with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:\n",
    "    f.write(csv_data)\n",
    "    temp_csv_path = f.name\n",
    "\n",
    "# Read CSV file\n",
    "csv_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(temp_csv_path)\n",
    "print(\"âœ… DataFrame from CSV:\")\n",
    "csv_df.show()\n",
    "\n",
    "# Clean up\n",
    "os.unlink(temp_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8ad38f",
   "metadata": {},
   "source": [
    "## 3. Complete Guide to All Types of Joins\n",
    "\n",
    "### Master every join type with practical examples and optimization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb0b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for join demonstrations\n",
    "print(\"ðŸ”— Comprehensive Join Operations Guide\\n\")\n",
    "\n",
    "# Employee performance data\n",
    "performance_data = [\n",
    "    (1, \"John Doe\", 85, \"Excellent\"),\n",
    "    (2, \"Jane Smith\", 78, \"Good\"),  \n",
    "    (3, \"Mike Johnson\", 92, \"Outstanding\"),\n",
    "    (5, \"David Brown\", 88, \"Excellent\"),\n",
    "    (9, \"New Employee\", 75, \"Good\")  # Employee not in main table\n",
    "]\n",
    "\n",
    "performance_df = spark.createDataFrame(performance_data, \n",
    "    [\"employee_id\", \"emp_name\", \"performance_score\", \"rating\"])\n",
    "\n",
    "# Department mapping\n",
    "dept_mapping = [\n",
    "    (\"Engineering\", \"TECH\"),\n",
    "    (\"Marketing\", \"MKTG\"), \n",
    "    (\"HR\", \"HUMAN_RESOURCES\"),\n",
    "    (\"Finance\", \"FIN\"),\n",
    "    (\"Operations\", \"OPS\")  # Department not in employees table\n",
    "]\n",
    "\n",
    "dept_codes_df = spark.createDataFrame(dept_mapping, [\"department\", \"dept_code\"])\n",
    "\n",
    "print(\"ðŸ“Š Source DataFrames:\")\n",
    "print(\"\\n1. Employees:\")\n",
    "employees_df.show()\n",
    "print(\"\\n2. Performance:\")\n",
    "performance_df.show()\n",
    "print(\"\\n3. Department Codes:\")\n",
    "dept_codes_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e52b513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. INNER JOIN - Returns only matching records from both tables\n",
    "print(\"ðŸ”— 1. INNER JOIN\")\n",
    "print(\"Returns only employees who have performance records\")\n",
    "inner_join = employees_df.join(performance_df, \"employee_id\", \"inner\")\n",
    "inner_join.show()\n",
    "print(f\"Result count: {inner_join.count()}\")\n",
    "\n",
    "# 2. LEFT JOIN (LEFT OUTER) - All records from left table\n",
    "print(\"\\nðŸ”— 2. LEFT JOIN (LEFT OUTER)\")\n",
    "print(\"All employees, with performance data where available\")\n",
    "left_join = employees_df.join(performance_df, \"employee_id\", \"left\")\n",
    "left_join.show()\n",
    "print(f\"Result count: {left_join.count()}\")\n",
    "\n",
    "# 3. RIGHT JOIN (RIGHT OUTER) - All records from right table  \n",
    "print(\"\\nðŸ”— 3. RIGHT JOIN (RIGHT OUTER)\")\n",
    "print(\"All performance records, with employee data where available\")\n",
    "right_join = employees_df.join(performance_df, \"employee_id\", \"right\")\n",
    "right_join.show()\n",
    "print(f\"Result count: {right_join.count()}\")\n",
    "\n",
    "# 4. FULL OUTER JOIN - All records from both tables\n",
    "print(\"\\nðŸ”— 4. FULL OUTER JOIN\")\n",
    "print(\"All employees and all performance records\")\n",
    "full_join = employees_df.join(performance_df, \"employee_id\", \"outer\")\n",
    "full_join.show()\n",
    "print(f\"Result count: {full_join.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936fd3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. SEMI JOIN - Returns left table records that have matches in right table\n",
    "print(\"\\nðŸ”— 5. SEMI JOIN\")\n",
    "print(\"Employees who have performance records (no columns from right table)\")\n",
    "semi_join = employees_df.join(performance_df, \"employee_id\", \"semi\")\n",
    "semi_join.show()\n",
    "print(f\"Result count: {semi_join.count()}\")\n",
    "\n",
    "# 6. ANTI JOIN - Returns left table records that DON'T have matches in right table\n",
    "print(\"\\nðŸ”— 6. ANTI JOIN\") \n",
    "print(\"Employees who DON'T have performance records\")\n",
    "anti_join = employees_df.join(performance_df, \"employee_id\", \"anti\")\n",
    "anti_join.show()\n",
    "print(f\"Result count: {anti_join.count()}\")\n",
    "\n",
    "# 7. CROSS JOIN - Cartesian product of both tables\n",
    "print(\"\\nðŸ”— 7. CROSS JOIN (Use with caution!)\")\n",
    "print(\"Every employee paired with every department code\")\n",
    "# Using small dataset for demonstration\n",
    "small_emp = employees_df.limit(2)\n",
    "small_dept = dept_codes_df.limit(2)\n",
    "cross_join = small_emp.crossJoin(small_dept)\n",
    "cross_join.show()\n",
    "print(f\"Result count: {cross_join.count()}\")\n",
    "\n",
    "# 8. MULTIPLE COLUMN JOIN\n",
    "print(\"\\nðŸ”— 8. MULTIPLE COLUMN JOIN\")\n",
    "print(\"Join on multiple conditions\")\n",
    "multi_join = employees_df.join(\n",
    "    performance_df, \n",
    "    (employees_df.employee_id == performance_df.employee_id) & \n",
    "    (employees_df.name == performance_df.emp_name),\n",
    "    \"inner\"\n",
    ")\n",
    "multi_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f525eaa2",
   "metadata": {},
   "source": [
    "## 4. Join Optimization Techniques\n",
    "\n",
    "### Master broadcast joins, bucketing, and performance tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911ae36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. BROADCAST JOIN - Optimize joins with small tables\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "print(\"ðŸš€ Join Optimization Techniques\\n\")\n",
    "\n",
    "# Broadcast the smaller department codes table\n",
    "print(\"1. BROADCAST JOIN:\")\n",
    "print(\"Broadcasting small department table for faster joins\")\n",
    "broadcast_join = employees_df.join(\n",
    "    broadcast(dept_codes_df), \n",
    "    \"department\", \n",
    "    \"left\"\n",
    ")\n",
    "broadcast_join.show()\n",
    "\n",
    "# Check execution plan\n",
    "print(\"\\nðŸ“Š Execution Plan Analysis:\")\n",
    "broadcast_join.explain(True)\n",
    "\n",
    "# 2. BUCKETING for large datasets (conceptual example)\n",
    "print(\"\\n2. BUCKETING STRATEGY:\")\n",
    "print(\"For large datasets, bucket tables on join keys\")\n",
    "\n",
    "# Example of writing bucketed table (commented for demo)\n",
    "# employees_df.write \\\n",
    "#     .bucketBy(4, \"employee_id\") \\\n",
    "#     .sortBy(\"employee_id\") \\\n",
    "#     .saveAsTable(\"bucketed_employees\")\n",
    "\n",
    "# 3. Join hints for query optimization\n",
    "print(\"\\n3. JOIN HINTS:\")\n",
    "print(\"Using join hints to guide optimizer\")\n",
    "\n",
    "# Different join strategies\n",
    "strategies = [\"BROADCAST\", \"MERGE\", \"SHUFFLE_HASH\"]\n",
    "for strategy in strategies:\n",
    "    print(f\"\\n{strategy} join strategy:\")\n",
    "    try:\n",
    "        hint_join = employees_df.hint(strategy).join(dept_codes_df, \"department\")\n",
    "        print(f\"âœ… {strategy} join executed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {strategy} join failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248890dc",
   "metadata": {},
   "source": [
    "## 5. Sorting Techniques and Partitioning Strategies\n",
    "\n",
    "### Master data ordering, partitioning, and distribution optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ccda15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“ˆ Sorting and Partitioning Techniques\\n\")\n",
    "\n",
    "# 1. SORTING TECHNIQUES\n",
    "print(\"1. SORTING TECHNIQUES:\")\n",
    "\n",
    "# Basic sorting\n",
    "print(\"\\nA. Basic sorting by salary (descending):\")\n",
    "sorted_by_salary = employees_df.orderBy(col(\"salary\").desc())\n",
    "sorted_by_salary.show()\n",
    "\n",
    "# Multiple column sorting\n",
    "print(\"\\nB. Multiple column sorting (department asc, salary desc):\")\n",
    "multi_sort = employees_df.orderBy(\"department\", col(\"salary\").desc())\n",
    "multi_sort.show()\n",
    "\n",
    "# Sort with null handling\n",
    "print(\"\\nC. Sort with null handling:\")\n",
    "# Add some null values for demonstration\n",
    "employees_with_nulls = employees_df.union(\n",
    "    spark.createDataFrame([(999, \"Test User\", None, 50000, \"2023-01-01\")], employees_schema)\n",
    ")\n",
    "null_sort = employees_with_nulls.orderBy(col(\"department\").asc_nulls_last())\n",
    "null_sort.show()\n",
    "\n",
    "# 2. PARTITIONING STRATEGIES\n",
    "print(\"\\n2. PARTITIONING STRATEGIES:\")\n",
    "\n",
    "# Check current partitions\n",
    "print(f\"\\nA. Current partitions: {employees_df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Repartition by column for better data locality\n",
    "print(\"\\nB. Partition by department:\")\n",
    "partitioned_df = employees_df.repartition(\"department\")\n",
    "print(f\"Partitions after repartitioning: {partitioned_df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Coalesce to reduce partitions\n",
    "print(\"\\nC. Coalesce to reduce partitions:\")\n",
    "coalesced_df = employees_df.coalesce(2)\n",
    "print(f\"Partitions after coalescing: {coalesced_df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# 3. PARTITION ANALYSIS\n",
    "print(\"\\n3. PARTITION ANALYSIS:\")\n",
    "def analyze_partitions(df, name):\n",
    "    partitions = df.rdd.glom().collect()\n",
    "    print(f\"\\n{name} partition analysis:\")\n",
    "    for i, partition in enumerate(partitions):\n",
    "        print(f\"Partition {i}: {len(partition)} records\")\n",
    "        if partition:\n",
    "            print(f\"  Sample: {partition[0]}\")\n",
    "\n",
    "analyze_partitions(employees_df, \"Original\")\n",
    "analyze_partitions(partitioned_df, \"Repartitioned by department\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90289c14",
   "metadata": {},
   "source": [
    "## 6. Performance Optimization Parameters\n",
    "\n",
    "### Complete guide to Spark configuration and tuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43be16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"âš¡ Performance Optimization Parameters Guide\\n\")\n",
    "\n",
    "# 1. MEMORY OPTIMIZATION\n",
    "print(\"1. MEMORY OPTIMIZATION PARAMETERS:\")\n",
    "memory_configs = {\n",
    "    \"spark.executor.memory\": \"Amount of memory per executor\",\n",
    "    \"spark.executor.memoryFraction\": \"Fraction for RDD storage/cache\", \n",
    "    \"spark.storage.memoryFraction\": \"Fraction for storage\",\n",
    "    \"spark.executor.memoryOffHeap.enabled\": \"Enable off-heap memory\",\n",
    "    \"spark.executor.memoryOffHeap.size\": \"Off-heap memory size\"\n",
    "}\n",
    "\n",
    "for param, description in memory_configs.items():\n",
    "    try:\n",
    "        value = spark.conf.get(param)\n",
    "        print(f\"âœ… {param}: {value} - {description}\")\n",
    "    except:\n",
    "        print(f\"âŒ {param}: Not set - {description}\")\n",
    "\n",
    "# 2. PARALLELISM PARAMETERS\n",
    "print(\"\\n2. PARALLELISM OPTIMIZATION:\")\n",
    "parallelism_configs = {\n",
    "    \"spark.sql.shuffle.partitions\": \"Partitions for shuffles\",\n",
    "    \"spark.sql.adaptive.coalescePartitions.enabled\": \"Auto coalesce partitions\",\n",
    "    \"spark.sql.adaptive.coalescePartitions.minPartitionNum\": \"Min partitions after coalesce\",\n",
    "    \"spark.sql.files.maxPartitionBytes\": \"Max bytes per partition when reading\",\n",
    "    \"spark.default.parallelism\": \"Default parallelism level\"\n",
    "}\n",
    "\n",
    "for param, description in parallelism_configs.items():\n",
    "    try:\n",
    "        value = spark.conf.get(param)\n",
    "        print(f\"âœ… {param}: {value} - {description}\")\n",
    "    except:\n",
    "        print(f\"âŒ {param}: Not set - {description}\")\n",
    "\n",
    "# 3. JOIN OPTIMIZATION PARAMETERS\n",
    "print(\"\\n3. JOIN OPTIMIZATION PARAMETERS:\")\n",
    "join_configs = {\n",
    "    \"spark.sql.autoBroadcastJoinThreshold\": \"Threshold for broadcast joins\",\n",
    "    \"spark.sql.adaptive.skewJoin.enabled\": \"Handle skewed joins\",\n",
    "    \"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\": \"Skew threshold\",\n",
    "    \"spark.sql.bucketing.coalesceBucketsInJoin.enabled\": \"Coalesce buckets in joins\"\n",
    "}\n",
    "\n",
    "for param, description in join_configs.items():\n",
    "    try:\n",
    "        value = spark.conf.get(param)\n",
    "        print(f\"âœ… {param}: {value} - {description}\")\n",
    "    except:\n",
    "        print(f\"âŒ {param}: Not set - {description}\")\n",
    "\n",
    "# 4. DYNAMIC OPTIMIZATION\n",
    "print(\"\\n4. ADAPTIVE QUERY EXECUTION (AQE):\")\n",
    "aqe_configs = {\n",
    "    \"spark.sql.adaptive.enabled\": \"Enable AQE\",\n",
    "    \"spark.sql.adaptive.localShuffleReader.enabled\": \"Local shuffle reader\", \n",
    "    \"spark.sql.adaptive.skewJoin.enabled\": \"Skew join optimization\",\n",
    "    \"spark.sql.adaptive.coalescePartitions.enabled\": \"Coalesce partitions\"\n",
    "}\n",
    "\n",
    "for param, description in aqe_configs.items():\n",
    "    try:\n",
    "        value = spark.conf.get(param)\n",
    "        print(f\"âœ… {param}: {value} - {description}\")\n",
    "    except:\n",
    "        print(f\"âŒ {param}: Not set - {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b77146a",
   "metadata": {},
   "source": [
    "## 7. Advanced Topics and Best Practices\n",
    "\n",
    "### Window functions, UDFs, caching, and performance monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7135d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ¯ Advanced PySpark Techniques\\n\")\n",
    "\n",
    "# 1. WINDOW FUNCTIONS\n",
    "print(\"1. WINDOW FUNCTIONS:\")\n",
    "\n",
    "# Row number within department\n",
    "window_dept = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "employees_with_rank = employees_df.withColumn(\"rank_in_dept\", row_number().over(window_dept))\n",
    "print(\"\\nEmployees ranked by salary within department:\")\n",
    "employees_with_rank.show()\n",
    "\n",
    "# Running total and lag/lead functions\n",
    "window_salary = Window.orderBy(\"salary\")\n",
    "advanced_window = employees_df.withColumn(\"running_total\", sum(\"salary\").over(window_salary)) \\\n",
    "                              .withColumn(\"prev_salary\", lag(\"salary\", 1).over(window_salary)) \\\n",
    "                              .withColumn(\"next_salary\", lead(\"salary\", 1).over(window_salary))\n",
    "print(\"\\nRunning totals and lag/lead:\")\n",
    "advanced_window.show()\n",
    "\n",
    "# 2. CACHING STRATEGIES\n",
    "print(\"\\n2. CACHING STRATEGIES:\")\n",
    "\n",
    "# Cache frequently used DataFrame\n",
    "employees_df.cache()\n",
    "print(f\"âœ… DataFrame cached. Storage level: {employees_df.storageLevel}\")\n",
    "\n",
    "# Different storage levels\n",
    "from pyspark import StorageLevel\n",
    "cached_memory_only = employees_df.persist(StorageLevel.MEMORY_ONLY)\n",
    "cached_memory_disk = employees_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "print(\"âœ… Multiple cache levels applied\")\n",
    "\n",
    "# 3. USER DEFINED FUNCTIONS (UDFs)\n",
    "print(\"\\n3. USER DEFINED FUNCTIONS:\")\n",
    "\n",
    "# Create a UDF for salary grade\n",
    "def salary_grade(salary):\n",
    "    if salary >= 80000:\n",
    "        return \"Senior\"\n",
    "    elif salary >= 65000:\n",
    "        return \"Mid\"\n",
    "    else:\n",
    "        return \"Junior\"\n",
    "\n",
    "# Register UDF\n",
    "from pyspark.sql.types import StringType\n",
    "salary_grade_udf = udf(salary_grade, StringType())\n",
    "\n",
    "# Apply UDF\n",
    "employees_with_grade = employees_df.withColumn(\"grade\", salary_grade_udf(\"salary\"))\n",
    "print(\"\\nEmployees with salary grades:\")\n",
    "employees_with_grade.show()\n",
    "\n",
    "# 4. AGGREGATIONS AND GROUPING\n",
    "print(\"\\n4. ADVANCED AGGREGATIONS:\")\n",
    "\n",
    "# Complex aggregations\n",
    "dept_stats = employees_df.groupBy(\"department\") \\\n",
    "    .agg(\n",
    "        count(\"employee_id\").alias(\"emp_count\"),\n",
    "        avg(\"salary\").alias(\"avg_salary\"),\n",
    "        min(\"salary\").alias(\"min_salary\"),\n",
    "        max(\"salary\").alias(\"max_salary\"),\n",
    "        stddev(\"salary\").alias(\"salary_stddev\")\n",
    "    )\n",
    "print(\"\\nDepartment statistics:\")\n",
    "dept_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55c265c",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Summary and Next Steps\n",
    "\n",
    "### What you've learned in this comprehensive tutorial:\n",
    "\n",
    "#### âœ… **Core Concepts Mastered:**\n",
    "- **SparkSession Configuration** - Optimized setup with performance parameters\n",
    "- **DataFrame Creation** - Multiple sources and schema management\n",
    "- **All Join Types** - Inner, Outer (Left/Right/Full), Semi, Anti, Cross joins\n",
    "- **Join Optimization** - Broadcast joins, bucketing, and join hints\n",
    "- **Sorting & Partitioning** - OrderBy, repartition, coalesce strategies\n",
    "- **Performance Tuning** - Memory, parallelism, and AQE parameters\n",
    "- **Advanced Operations** - Window functions, UDFs, and caching\n",
    "\n",
    "#### ðŸš€ **Optimization Techniques Covered:**\n",
    "- Adaptive Query Execution (AQE)\n",
    "- Broadcast joins for small tables\n",
    "- Partition pruning and bucketing\n",
    "- Memory management and garbage collection\n",
    "- Cache strategies and storage levels\n",
    "- Skew join handling\n",
    "\n",
    "#### ðŸ“Š **Key Parameters Learned:**\n",
    "```\n",
    "spark.sql.adaptive.enabled = true\n",
    "spark.sql.autoBroadcastJoinThreshold = 10MB\n",
    "spark.sql.shuffle.partitions = 200\n",
    "spark.sql.adaptive.skewJoin.enabled = true\n",
    "spark.sql.adaptive.coalescePartitions.enabled = true\n",
    "```\n",
    "\n",
    "#### ðŸŽ¯ **Next Learning Path:**\n",
    "1. **Specialized Notebooks** - Dive deeper into specific topics\n",
    "2. **Real-world Projects** - Apply skills on large datasets  \n",
    "3. **Streaming Data** - Structured Streaming for real-time processing\n",
    "4. **MLlib Integration** - Machine learning with Spark\n",
    "5. **Delta Lake** - Advanced data lake management\n",
    "\n",
    "#### ðŸ’¡ **Best Practices to Remember:**\n",
    "- Always use schemas for better performance\n",
    "- Cache DataFrames that are used multiple times\n",
    "- Use broadcast joins for small lookup tables\n",
    "- Monitor execution plans with `.explain()`\n",
    "- Partition data by frequently filtered columns\n",
    "- Use AQE for dynamic optimization\n",
    "\n",
    "### Ready to become a PySpark expert? Let's continue with specialized topics! ðŸ”¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74eb47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up resources\n",
    "print(\"ðŸ§¹ Cleaning up resources...\")\n",
    "\n",
    "# Unpersist cached DataFrames\n",
    "employees_df.unpersist()\n",
    "print(\"âœ… Cache cleared\")\n",
    "\n",
    "# Stop SparkSession (optional - usually done at end of application)\n",
    "# spark.stop()\n",
    "print(\"âœ… Session ready for next operations\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Congratulations! You've completed the comprehensive PySpark tutorial!\")\n",
    "print(\"ðŸ“š Ready for the next advanced topics in the series!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
